{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bba11b8",
   "metadata": {},
   "source": [
    "# Data Description\n",
    "\n",
    "Let's take a moment to explain what these values are and why they're important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73feed6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "\n",
    "from sklearn.decomposition import PCA,TruncatedSVD\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "from scipy.stats import skew"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b035817",
   "metadata": {},
   "source": [
    "## Data Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "644232d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('pickles/df.pkl', 'rb') as f:\n",
    "#     df = pickle.load(f)\n",
    "# with open('pickles/df_librosa.pkl', 'rb') as f:\n",
    "#     df_librosa = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cf30f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aaf8635a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_librosa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_librosa \u001b[38;5;241m=\u001b[39m \u001b[43mdf_librosa\u001b[49m\u001b[38;5;241m.\u001b[39mdropna()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_librosa' is not defined"
     ]
    }
   ],
   "source": [
    "df_librosa = df_librosa.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60405250",
   "metadata": {},
   "source": [
    "##  Audio Metrics Description\n",
    "\n",
    "From the Librosa Library, we imported several important metrics that will help our model distinguish between human speech and computer generated speech. I'd like to define what these metrics are and why I believe that they are important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc23bf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = 'datasets/somos/audios/LJ028-0146_027.wav'\n",
    "y,sr = librosa.load(audio_file,sr=44100)\n",
    "audio_data = librosa.effects.preemphasis(y, coef=0.97)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d4157d",
   "metadata": {},
   "source": [
    "#### Mel-Frequency Cepstral Coefficients (MFCCs)\n",
    "\n",
    "What they are: MFCCs stands for Mel-Frequency Cepstral Coefficients, which are a type of feature extraction technique commonly used in audio processing and machine learning applications. MFCCs are based on the human ear's perception of sound, and they represent the spectral envelope of a sound signal in a compressed form. They are used to extract important features from audio signals that can then be used in machine learning models to classify or recognize sound\n",
    "\n",
    "What we're looking for: This metric provides us with the shape of an individual audio file. Within each audio file, it will be important to determine if the shape of computer generated speech is different than human speech."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964b699a",
   "metadata": {},
   "source": [
    "For this metric, it's important to consider whether or not the data you're working with is highly correlated. Since we are working with speech data, we will assume that the data is highly correlated. This is because speech differs significantly from other audio signals in that it is:\n",
    "\n",
    "1. Contextual and time dependent: \n",
    "    - The meaning of the second word is dependent on the first word)\n",
    "2. Contains a mix of harmonic inharmonic components:\n",
    "    - The human vocal tract produces sounds on a much wider spectrum than an instrument would\n",
    "    - An instrument would produce a less variable output than the human vocal tract\n",
    "    \n",
    "As a result of these two complexities, it's likely our model will be highly susceptible to highly correlated features. We'll utilize MFCCs and opt to mean-normalize our data. This is en lieu of Mel-Scaled Filter banks and using the built in normalization feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c187bc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfccs = (librosa.feature.mfcc(audio_data, sr, n_mels=40, n_fft=2048, hop_length=512, win_length=1024))\n",
    "mfccs_mean = np.mean(mfccs, axis=1)\n",
    "mfccs_norm = mfccs - mfccs_mean[:, np.newaxis]\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.specshow(mfccs_norm, x_axis=\"time\", y_axis=\"mel\", sr=sr)\n",
    "plt.colorbar()\n",
    "plt.title('MFCCs-- Generated')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('MFCCs')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd24202",
   "metadata": {},
   "source": [
    "Above is a representation of a computer generation audio clip. We'll do the same below for a human utterance using the same UtteranceId (sentence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140edaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file_human = 'datasets/somos/audios/gt_LJ028-0146_000.wav'\n",
    "y,sr = librosa.load(audio_file_human,sr=44100)\n",
    "audio_data_human = librosa.effects.preemphasis(y, coef=0.97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd0bbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfccs = (librosa.feature.mfcc(audio_data_human, sr, n_mels=40, n_fft=2048, hop_length=512, win_length=1024))\n",
    "mfccs_norm = mfccs - mfccs_mean[:, np.newaxis]\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.specshow(mfccs_norm, x_axis=\"time\", y_axis=\"mel\", sr=sr)\n",
    "plt.colorbar()\n",
    "plt.title('MFCCs-- Human')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('MFCCs')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1beb4f79",
   "metadata": {},
   "source": [
    "As you can see, There are differences between the human audio and the computer generated audio. This audio was generated using the same data between the human version and the computer version, but they produced a slightly different shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bb8bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac788be",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mfccs[0].reshape(-1,1)\n",
    "pca = PCA(n_components=1)\n",
    "pca.fit(X)\n",
    "X_pca = pca.transform(X)\n",
    "plt.plot(X_pca)\n",
    "plt.plot(mfccs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a13e80",
   "metadata": {},
   "source": [
    "### SVD of MFCCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9447b6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# svd = TruncatedSVD(n_components=10)\n",
    "# X_svd = svd.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919ac201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"U:\\n\", U)\n",
    "# print(\"s:\\n\", s)\n",
    "# print(\"V:\\n\", V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482d81fd",
   "metadata": {},
   "source": [
    "#### Spectral Centroids\n",
    "\n",
    "Spectral centroids can tell us about the \"brightness\" or \"darkness\" of a sound in a particular frequency range. In the case of voice data, we can use spectral centroids to analyze the characteristics of a speaker's voice. For example, a high spectral centroid could indicate that a speaker has a \"brighter\" voice with more high-frequency components, while a low spectral centroid could indicate a \"darker\" voice with more low-frequency components. \n",
    "\n",
    "Additionally, we can track changes in the spectral centroid over time to analyze how a speaker's voice changes or how their tone and emotion fluctuate throughout a conversation. Overall, spectral centroids can provide valuable insights into the characteristics and qualities of a speaker's voice, which can be useful for tasks such as speaker identification, emotion recognition, and speech analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d73089",
   "metadata": {},
   "source": [
    "Once again, we'll calculate these values for the generated audio and then again for the human audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e1f0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = 'datasets/somos/audios/LJ028-0146_027.wav'\n",
    "y,sr = librosa.load(audio_file,sr=44100)\n",
    "audio_data = librosa.effects.preemphasis(y, coef=0.97)\n",
    "np.isnan(audio_data).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc57991",
   "metadata": {},
   "outputs": [],
   "source": [
    "spectral_centroids = librosa.feature.spectral_centroid(audio_data, sr = 44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83856bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file_human = 'datasets/somos/audios/gt_LJ028-0146_000.wav'\n",
    "y,sr = librosa.load(audio_file_human,sr=44100)\n",
    "audio_data_human = librosa.effects.preemphasis(y, coef=0.97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6454b5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spectral_centroids_human = librosa.feature.spectral_centroid(audio_data_human, sr = 44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0065d0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = librosa.times_like(spectral_centroids_human)\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 1, 1)\n",
    "plt.plot(spectral_centroids_human.T, label='Spectral centroid (Human)')\n",
    "plt.plot(spectral_centroids.T, label='Spectral centroid (Generated)')\n",
    "plt.ylabel('Hz')\n",
    "plt.xlabel('Time (ms)')\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aec4a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(spectral_centroids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da2e517",
   "metadata": {},
   "source": [
    "Given the same sentence, both the human and computer generated audio produced a similar spectrogram. This is a reasonable result as the spectral centroids tell us 'how' a sentence is said. The higher the spectral value, the brighter the tone; the lower, the darker the tone. The fluctuations in the graph indicate the natural fluctuations of speech. It's expected that the spectrogram will vary from person to person for the same sentence, so it's not unexpected to have a similar result.\n",
    "\n",
    "Over a series of calculations for this result, it is possible that there are trends within the differences in computer vs. Human speech that we do not see within this single graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828bc74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = spectral_centroids_human[0].reshape(-1,1)\n",
    "pca = PCA(n_components=1)\n",
    "pca.fit(X)\n",
    "X_pca = pca.transform(X)\n",
    "len(X_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142de558",
   "metadata": {},
   "source": [
    "#### Spectral Bandwidth\n",
    "\n",
    "In the context of speech, spectral bandwidth can provide information about the overall spectral content of a speech signal. Specifically, it gives us an idea of how wide the frequency range of the speech signal is at any given time, which can be useful for various speech processing tasks such as speech recognition, speaker identification, and emotion recognition. \n",
    "\n",
    "For example, changes in spectral bandwidth over time can indicate changes in the vocal tract shape and size, which can be indicative of changes in speech style or emotional state. Additionally, spectral bandwidth can be used as a feature for speech recognition, where different phonemes are known to have different bandwidth characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbc2560",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = 'datasets/somos/audios/LJ028-0146_027.wav'\n",
    "y,sr = librosa.load(audio_file,sr=44100)\n",
    "audio_data = librosa.effects.preemphasis(y, coef=0.97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7726604",
   "metadata": {},
   "outputs": [],
   "source": [
    "spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e1d3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file_human = 'datasets/somos/audios/gt_LJ028-0146_000.wav'\n",
    "y,sr = librosa.load(audio_file_human,sr=44100)\n",
    "audio_data_human = librosa.effects.preemphasis(y, coef=0.97)\n",
    "S = np.abs(librosa.stft(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8b540e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spectral_bandwidth_human = librosa.feature.spectral_bandwidth(y=y, sr=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7651cf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, sharex=True)\n",
    "times = librosa.times_like(spectral_bandwidth)\n",
    "centroid = librosa.feature.spectral_centroid(S=S)\n",
    "ax[0].semilogy(times, spectral_bandwidth[0], label='Spectral bandwidth')\n",
    "ax[0].set(ylabel='Hz', xticks=[], xlim=[times.min(), times.max()])\n",
    "ax[0].legend()\n",
    "ax[0].label_outer()\n",
    "librosa.display.specshow(librosa.amplitude_to_db(S, ref=np.max),\n",
    "                         y_axis='log', x_axis='time', ax=ax[1])\n",
    "ax[1].set(title='log Power spectrogram')\n",
    "ax[1].fill_between(times, np.maximum(0, centroid[0] - (spectral_bandwidth[0,:len(centroid.T)])),\n",
    "                np.minimum(centroid[0] + spectral_bandwidth[0], sr/2),\n",
    "                alpha=0.5, label='Centroid +- bandwidth')\n",
    "ax[1].plot(times, centroid[0], label='Spectral centroid', color='w')\n",
    "#ax[1].legend(loc='lower left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd6c258",
   "metadata": {},
   "source": [
    "Because the clips are so short, the full effect of change in timbre is likely to not be seen within this plot. As a person speaks, the timbre of their voice varies as they project emotion in their voices or change their tone. With sentences that are typically less than 10 seconds, this feature will likely not be able to show significant results immediately.\n",
    "\n",
    "Similar to the spectral centroids, it's likely that the variance within each plot will yield the biggest difference. As we can see in the above plot, the fluctuations are similar, however there are observable differences in the local highs and lows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351290f8",
   "metadata": {},
   "source": [
    "#### Spectral Contrast\n",
    "\n",
    "Spectral contrast is a method for analyzing the spectral content of audio signals. It measures the difference in magnitude between peaks and valleys in the frequency spectrum of an audio signal. The spectral contrast is computed by dividing the frequency spectrum into a number of sub-bands and comparing the mean magnitudes of the peaks and valleys within each sub-band.\n",
    "\n",
    "While spectral centroid and bandwidth describe the center and spread of the frequency content of a signal, spectral contrast measures the differences between spectral peaks and valleys in each frequency band. In other words, spectral contrast measures how much a given frequency bin differs from its surrounding bins in terms of magnitude.\n",
    "\n",
    "Spectral contrast is often used in speech recognition and music genre classification tasks, as it can provide information about the relative prominence of different frequency components in a signal, and how they contribute to the overall perceptual qualities of the signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb4afb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = 'datasets/somos/audios/LJ028-0146_027.wav'\n",
    "y,sr = librosa.load(audio_file,sr=44100)\n",
    "audio_data = librosa.effects.preemphasis(y, coef=0.97)\n",
    "duration = librosa.get_duration(y=y, sr=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5b050e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e21c674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute spectral contrast and spectral centroid\n",
    "spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
    "spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "\n",
    "# Plot the spectral contrast\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.specshow(spectral_contrast, x_axis='time', sr=sr, hop_length= 512)\n",
    "plt.colorbar(format='%+0.1f dB')\n",
    "plt.title('Spectral Contrast Generated')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Frequency band')\n",
    "plt.xlim(0,5.8)\n",
    "plt.ylim(-.6,5)\n",
    "\n",
    "# Overlay the spectral centroid\n",
    "plt.plot(librosa.times_like(spectral_centroids), spectral_centroids[0], color='w', linewidth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a0312a",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file_human = 'datasets/somos/audios/gt_LJ028-0146_000.wav'\n",
    "y,sr = librosa.load(audio_file_human,sr=44100)\n",
    "audio_data_human = librosa.effects.preemphasis(y, coef=0.97)\n",
    "S = np.abs(librosa.stft(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2329f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute spectral contrast and spectral centroid\n",
    "spectral_contrast_human = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
    "spectral_centroids_human = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "\n",
    "# Plot the spectral contrast\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.specshow(spectral_contrast_human, x_axis='time', sr=sr, hop_length= 512)\n",
    "plt.colorbar(format='%+0.1f dB')\n",
    "plt.title('Spectral Contrast Human')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Frequency band')\n",
    "plt.xlim(0,5.8)\n",
    "plt.ylim(-.6,5)\n",
    "\n",
    "# Overlay the spectral centroid\n",
    "plt.plot(librosa.times_like(spectral_centroids_human), spectral_centroids_human[0], color='w', linewidth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1a71b9",
   "metadata": {},
   "source": [
    "#### Zero Crossing Rate\n",
    "\n",
    "In speech analysis, zero crossing rate refers to the rate at which the speech signal changes from positive to negative or vice versa. It is a measure of the frequency content of a speech signal, and is related to the pitch and the vocal timbre of the speaker. High zero crossing rate generally indicates high-frequency content in the speech signal, while low zero crossing rate indicates low-frequency content. Zero crossing rate can be used as a feature in speech recognition and speaker identification systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02330a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = 'datasets/somos/audios/LJ028-0146_027.wav'\n",
    "y,sr = librosa.load(audio_file,sr=44100)\n",
    "audio_data = librosa.effects.preemphasis(y, coef=0.97)\n",
    "duration = librosa.get_duration(y=y, sr=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce36d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_crossing_rate = librosa.feature.zero_crossing_rate(audio_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952b2177",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.linspace(0, duration, num=zero_crossing_rate.shape[1])\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(t, zero_crossing_rate[0], label='ZCR')\n",
    "plt.title('Zero Crossing Rate Generated')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('ZCR')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ebd63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_crossing_rate.min(), zero_crossing_rate.max(), zero_crossing_rate.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b79e381",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file_human = 'datasets/somos/audios/gt_LJ028-0146_000.wav'\n",
    "y,sr = librosa.load(audio_file_human,sr=44100)\n",
    "audio_data_human = librosa.effects.preemphasis(y, coef=0.97)\n",
    "S = np.abs(librosa.stft(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d014f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_crossing_rate_human = librosa.feature.zero_crossing_rate(audio_data_human)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8f9ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.linspace(0, duration, num=zero_crossing_rate_human.shape[1])\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(t, zero_crossing_rate_human[0], label='ZCR')\n",
    "plt.title('Zero Crossing Rate Human')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('ZCR')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50df044a",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_crossing_rate_human.min(), zero_crossing_rate_human.max(), zero_crossing_rate_human.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afd3e05",
   "metadata": {},
   "source": [
    "The results from both calculations do not yield significant visible results in the two plots. We'll see if this feature is important later in the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706d5c33",
   "metadata": {},
   "source": [
    "#### Chroma\n",
    "\n",
    "In voice recognition, Chroma is a feature that helps to identify and distinguish between different notes and pitches in a person's voice. By analyzing the frequency spectrum of a voice signal and mapping it onto a musical scale, Chroma can provide a representation of the harmonic content of a person's voice. This information can be useful for tasks such as speaker identification and music transcription, where it is important to accurately capture the nuances of a person's vocal performance. \n",
    "\n",
    "Additionally, since Chroma captures information about the underlying musical structure of a voice signal, it can be used to identify patterns and trends in a person's vocal behavior, which may be useful for studying speech and language development or identifying vocal disorders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2b92a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = 'datasets/somos/audios/LJ028-0146_027.wav'\n",
    "y,sr = librosa.load(audio_file,sr=44100)\n",
    "audio_data = librosa.effects.preemphasis(y, coef=0.97)\n",
    "duration = librosa.get_duration(y=y, sr=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e966dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_cens = librosa.feature.chroma_cens(y=y, sr=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639f1ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.specshow(chroma_cens, y_axis='chroma', x_axis='time')\n",
    "plt.colorbar()\n",
    "plt.title('Chromagram (Generated)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610619b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file_human = 'datasets/somos/audios/gt_LJ028-0146_000.wav'\n",
    "y,sr = librosa.load(audio_file_human,sr=44100)\n",
    "audio_data_human = librosa.effects.preemphasis(y, coef=0.97)\n",
    "S = np.abs(librosa.stft(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37497774",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_cens_human = librosa.feature.chroma_cens(y=y, sr=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a8af74",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.specshow(chroma_cens_human, y_axis='chroma', x_axis='time')\n",
    "plt.colorbar()\n",
    "plt.title('Chromagram (Human)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7073a708",
   "metadata": {},
   "source": [
    "This plot shows a significant difference between the generated and human iterations of the same sentence. Each horizontal bar represents a specific pitch class (note). As we move through the plot, from left to right, we see how that note changes within the audio signal.\n",
    "\n",
    "Within the human audio signal, we see a very clear path from one note to the next. In the generated audio, we do not see this same clear trend."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8b79fb",
   "metadata": {},
   "source": [
    "## Root Mean Squared\n",
    "\n",
    "Before we process our data, we want all of our audio to be normalized against the root mean squared amplitude. This is a measure of the energy in each audio signal, which we can interpret as loudness or volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1a574f",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = 'datasets/somos/audios/LJ028-0146_027.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fca08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y, sr = librosa.load(audio_file)\n",
    "rms = librosa.feature.rms(y=y)[0]\n",
    "maxrms,minrms = max(rms),min(rms)\n",
    "maxrms,minrms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed0acc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file_human = 'datasets/somos/audios/gt_LJ028-0146_000.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2faddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y, sr = librosa.load(audio_file_human)\n",
    "rms = librosa.feature.rms(y=y)[0]\n",
    "maxrms,minrms = max(rms),min(rms)\n",
    "maxrms,minrms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9552faf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pickles/df.pkl', 'wb') as f:\n",
    "    pickle.dump(df, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858b090f",
   "metadata": {},
   "source": [
    "### Additional Data\n",
    "\n",
    "Now we'll use what we found above to import additional data. We'll use the Librosa Library, which will be introduced in the next sheet."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
