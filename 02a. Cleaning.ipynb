{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86056cb7",
   "metadata": {},
   "source": [
    "# Cleaning\n",
    "\n",
    "This workbook is dedicated to the cleaning completed on the files within our dataset.\n",
    "\n",
    "Since we're working with audio files, much of the cleaning that will be done to the audio itself will be done within the Library call in the next section. This workbook provides the cleaning that was necessary for the filepaths to make programmatic access and labelling of each file, possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70bfbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "\n",
    "from sklearn.decomposition import PCA,TruncatedSVD\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "from scipy.stats import skew\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d1a573",
   "metadata": {},
   "source": [
    "### Moving Files\n",
    "\n",
    "Some of the files are deeply nested in sub-sub-sub folders. We'll programmatically walk through our dataset directory and pull all of the files into a general folder we can more easily access. In this, we'll also pull the metadata from each subfolder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070bd622",
   "metadata": {},
   "outputs": [],
   "source": [
    "Set the folder path\n",
    "folder_path = '/Users/ryan/GA/Projects/Capstone/datasets/Audio/'\n",
    "\n",
    "Define the new folder path\n",
    "new_folder_path = '/Users/ryan/GA/Projects/Capstone/datasets/AllFiles/'\n",
    "\n",
    "if not os.path.exists(new_folder_path):\n",
    "    os.makedirs(new_folder_path)\n",
    "\n",
    "# Iterate over all files in the original folder and its subfolders\n",
    "for root, dirs, files in os.walk(folder_path):\n",
    "    for filename in files:\n",
    "        # Get the full path of the source file\n",
    "        source_file = os.path.join(root, filename)\n",
    "        # Generate a unique file name by appending a timestamp\n",
    "        timestamp = datetime.datetime.now().strftime('%Y%m%d%H%M%S%f')\n",
    "        unique_filename = f'{os.path.splitext(filename)[0]}_{timestamp}{os.path.splitext(filename)[1]}'\n",
    "        # Check if the file already exists in the new folder\n",
    "        if os.path.exists(os.path.join(new_folder_path, unique_filename)):\n",
    "            print(f'File already exists: {unique_filename}')\n",
    "            continue\n",
    "        # Copy the file to the destination folder with the unique file name\n",
    "        destination_file = os.path.join(new_folder_path, unique_filename)\n",
    "        shutil.copy2(source_file, destination_file)\n",
    "        print(f'Copied file: {filename} to {destination_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c346f18c",
   "metadata": {},
   "source": [
    "### Duplicating Spectrogram Images\n",
    "\n",
    "We'll need to randomly sample the audio data, so let's combine compile all the spectrograms from each classes in their own folders labelled 'Human' and 'Synthetic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741da620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "src_dir = \"datasets/Spectrograms/fake/\"\n",
    "dst_dir = \"datasets/Spectrograms/SYNTHETIC/\"\n",
    "\n",
    "# Create the destination directory if it doesn't exist\n",
    "if not os.path.exists(dst_dir):\n",
    "    os.makedirs(dst_dir)\n",
    "\n",
    "# Walk through all subdirectories of the source directory\n",
    "for dirpath, dirnames, filenames in os.walk(src_dir):\n",
    "    # Copy all files to the destination directory\n",
    "    for filename in filenames:\n",
    "        # Get the source file path\n",
    "        src_path = os.path.join(dirpath, filename)\n",
    "        # Get the destination file path\n",
    "        dst_path = os.path.join(dst_dir, filename)\n",
    "        # Copy the file to the destination directory\n",
    "        shutil.copy2(src_path, dst_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0192b689",
   "metadata": {},
   "source": [
    "### Removing Duplicates\n",
    "\n",
    "Since we're working with many files, I'd like to remove duplicates by comparing the hashes of each file. Many of the files in our dataset have the same filename, which is the case when a dataset contains both real and synthetic audio for the same utterance.\n",
    "\n",
    "To avoid removing a file of this nature, we'll need to compare the hashes for all the files and remove only duplicate audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bf0702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "\n",
    "folder_path = '/Users/ryan/GA/Projects/Capstone/datasets/Audio/'\n",
    "\n",
    "# loop over all subdirectories in folder_path\n",
    "for subdir, _, files in os.walk(folder_path):\n",
    "    # create a dictionary to store the hash values of each file\n",
    "    hash_dict = {}\n",
    "\n",
    "    # loop over each file in the current subdirectory\n",
    "    for file in files:\n",
    "        # get the full path of the file\n",
    "        file_path = os.path.join(subdir, file)\n",
    "\n",
    "        # calculate the MD5 hash of the file\n",
    "        with open(file_path, 'rb') as f:\n",
    "            file_hash = hashlib.md5(f.read()).hexdigest()\n",
    "\n",
    "        # check if the hash value already exists in the dictionary\n",
    "        if file_hash in hash_dict.values():\n",
    "            # if so, delete the file\n",
    "            os.remove(file_path)\n",
    "        else:\n",
    "            # if not, add the hash value to the dictionary\n",
    "            hash_dict[file] = file_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92dd617",
   "metadata": {},
   "source": [
    "### Removing Empty Audio Files\n",
    "\n",
    "It's possible that an audio file in our dataset is empty, especially given we are working with unvalidated crowdsourced data from Mozilla. We'll iterate through the dataset and remove any files that are empty by using soundfile to check the audio output and measure the volume of each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3bdc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_dir = 'datasets/Audio/'\n",
    "\n",
    "# Walk through the directory tree and loop over all files in all subfolders\n",
    "import soundfile as sf\n",
    "\n",
    "audio_extensions = ['.wav', '.aiff', '.aif', '.flac', '.mp3']\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(top_dir):\n",
    "    for filename in filenames:\n",
    "        ext = os.path.splitext(filename)[-1].lower()\n",
    "        if ext in audio_extensions:\n",
    "            file_path = os.path.join(dirpath, filename)\n",
    "            try:\n",
    "                y, sr = sf.read(file_path, always_2d=True)\n",
    "                if y.size == 0:\n",
    "                    # Remove the file if it has an empty audio signal\n",
    "                    os.remove(file_path)\n",
    "                    print(f'Removed empty audio file: {file_path}')\n",
    "            except Exception as e:\n",
    "                print(f'Error loading audio file {file_path}: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d76e2d1",
   "metadata": {},
   "source": [
    "### Converting Mozilla to .wav\n",
    "\n",
    "The Mozilla dataset contains files that are of type .mp3, which will cause problems when we call the Librosa Library in the next section. We'll use Pydub to iterate through the Mozilla data and convert each file to .wav."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298615be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import os\n",
    "\n",
    "# Set the input and output directories\n",
    "input_dir = 'datasets/Audio/Mozilla_delta/'\n",
    "output_dir = 'datasets/Audio/Mozilla_delta(wav)//'\n",
    "\n",
    "# Loop through all .mp3 files in the input directory\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith('.mp3'):\n",
    "        # Load the audio file using pydub\n",
    "        audio = AudioSegment.from_file(os.path.join(input_dir, filename), format='mp3')\n",
    "\n",
    "        # Set the output filename and path\n",
    "        output_filename = filename[:-4] + '.wav' # Remove the '.mp3' extension and replace with '.wav'\n",
    "        output_path = os.path.join(output_dir, output_filename)\n",
    "\n",
    "        # Export the audio file in .wav format\n",
    "        audio.export(output_path, format='wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f60b3c9",
   "metadata": {},
   "source": [
    "### Image Compression\n",
    "\n",
    "To reduce the size of our image, we'll use PIL (Pillow) Fork to optimize the quality of our images. By setting quality to 85% and optimize = True, we shouldn't lose a considerable amount of data and we should be able to improve our model run time significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09fb851",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb456f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"datasets/Spectrograms/HUMAN/\"\n",
    "\n",
    "# Compression quality\n",
    "quality = 85\n",
    "\n",
    "# Count the number of files in the directory\n",
    "num_files = len(os.listdir(directory))\n",
    "\n",
    "# Iterate through the files with a progress bar\n",
    "for i, filename in tqdm(enumerate(os.listdir(directory)), total=num_files):\n",
    "    if filename.endswith(\".png\"):\n",
    "        img = Image.open(os.path.join(directory, filename))\n",
    "        img.save(os.path.join(directory, filename), optimize=True, quality=quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01999d62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
