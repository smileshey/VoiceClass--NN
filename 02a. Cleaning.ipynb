{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86056cb7",
   "metadata": {},
   "source": [
    "# Cleaning\n",
    "\n",
    "This workbook is dedicated to the cleaning completed on the files within our dataset.\n",
    "\n",
    "Since we're working with audio files, much of the cleaning that will be done to the audio itself will be done within the Library call in the next section. This workbook provides the cleaning that was necessary for the filepaths to make programmatic access and labelling of each file, possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d70bfbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "\n",
    "from sklearn.decomposition import PCA,TruncatedSVD\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "from scipy.stats import skew\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d1a573",
   "metadata": {},
   "source": [
    "### Moving Files\n",
    "\n",
    "Some of the files are deeply nested in sub-sub-sub folders. We'll programmatically walk through our dataset directory and pull all of the files into a general folder we can more easily access. In this, we'll also pull the metadata from each subfolder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070bd622",
   "metadata": {},
   "outputs": [],
   "source": [
    "Set the folder path\n",
    "folder_path = '/Users/ryan/GA/Projects/Capstone/datasets/Audio/'\n",
    "\n",
    "Define the new folder path\n",
    "new_folder_path = '/Users/ryan/GA/Projects/Capstone/datasets/AllFiles/'\n",
    "\n",
    "if not os.path.exists(new_folder_path):\n",
    "    os.makedirs(new_folder_path)\n",
    "\n",
    "# Iterate over all files in the original folder and its subfolders\n",
    "for root, dirs, files in os.walk(folder_path):\n",
    "    for filename in files:\n",
    "        # Get the full path of the source file\n",
    "        source_file = os.path.join(root, filename)\n",
    "        # Generate a unique file name by appending a timestamp\n",
    "        timestamp = datetime.datetime.now().strftime('%Y%m%d%H%M%S%f')\n",
    "        unique_filename = f'{os.path.splitext(filename)[0]}_{timestamp}{os.path.splitext(filename)[1]}'\n",
    "        # Check if the file already exists in the new folder\n",
    "        if os.path.exists(os.path.join(new_folder_path, unique_filename)):\n",
    "            print(f'File already exists: {unique_filename}')\n",
    "            continue\n",
    "        # Copy the file to the destination folder with the unique file name\n",
    "        destination_file = os.path.join(new_folder_path, unique_filename)\n",
    "        shutil.copy2(source_file, destination_file)\n",
    "        print(f'Copied file: {filename} to {destination_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c346f18c",
   "metadata": {},
   "source": [
    "### Duplicating Spectrogram Images\n",
    "\n",
    "We'll need to randomly sample the audio data, so let's combine compile all the spectrograms from each classes in their own folders labelled 'Human' and 'Synthetic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741da620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "src_dir = \"datasets/Spectrograms/fake/\"\n",
    "dst_dir = \"datasets/Spectrograms/SYNTHETIC/\"\n",
    "\n",
    "# Create the destination directory if it doesn't exist\n",
    "if not os.path.exists(dst_dir):\n",
    "    os.makedirs(dst_dir)\n",
    "\n",
    "# Walk through all subdirectories of the source directory\n",
    "for dirpath, dirnames, filenames in os.walk(src_dir):\n",
    "    # Copy all files to the destination directory\n",
    "    for filename in filenames:\n",
    "        # Get the source file path\n",
    "        src_path = os.path.join(dirpath, filename)\n",
    "        # Get the destination file path\n",
    "        dst_path = os.path.join(dst_dir, filename)\n",
    "        # Copy the file to the destination directory\n",
    "        shutil.copy2(src_path, dst_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0192b689",
   "metadata": {},
   "source": [
    "### Removing Duplicates\n",
    "\n",
    "Since we're working with many files, I'd like to remove duplicates by comparing the hashes of each file. Many of the files in our dataset have the same filename, which is the case when a dataset contains both real and synthetic audio for the same utterance.\n",
    "\n",
    "To avoid removing a file of this nature, we'll need to compare the hashes for all the files and remove only duplicate audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bf0702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "\n",
    "folder_path = '/Users/ryan/GA/Projects/Capstone/datasets/Audio/'\n",
    "\n",
    "# loop over all subdirectories in folder_path\n",
    "for subdir, _, files in os.walk(folder_path):\n",
    "    # create a dictionary to store the hash values of each file\n",
    "    hash_dict = {}\n",
    "\n",
    "    # loop over each file in the current subdirectory\n",
    "    for file in files:\n",
    "        # get the full path of the file\n",
    "        file_path = os.path.join(subdir, file)\n",
    "\n",
    "        # calculate the MD5 hash of the file\n",
    "        with open(file_path, 'rb') as f:\n",
    "            file_hash = hashlib.md5(f.read()).hexdigest()\n",
    "\n",
    "        # check if the hash value already exists in the dictionary\n",
    "        if file_hash in hash_dict.values():\n",
    "            # if so, delete the file\n",
    "            os.remove(file_path)\n",
    "        else:\n",
    "            # if not, add the hash value to the dictionary\n",
    "            hash_dict[file] = file_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92dd617",
   "metadata": {},
   "source": [
    "### Removing Empty Audio Files\n",
    "\n",
    "It's possible that an audio file in our dataset is empty, especially given we are working with unvalidated crowdsourced data from Mozilla. We'll iterate through the dataset and remove any files that are empty by using soundfile to check the audio output and measure the volume of each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3bdc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_dir = 'datasets/Audio/'\n",
    "\n",
    "# Walk through the directory tree and loop over all files in all subfolders\n",
    "import soundfile as sf\n",
    "\n",
    "audio_extensions = ['.wav', '.aiff', '.aif', '.flac', '.mp3']\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(top_dir):\n",
    "    for filename in filenames:\n",
    "        ext = os.path.splitext(filename)[-1].lower()\n",
    "        if ext in audio_extensions:\n",
    "            file_path = os.path.join(dirpath, filename)\n",
    "            try:\n",
    "                y, sr = sf.read(file_path, always_2d=True)\n",
    "                if y.size == 0:\n",
    "                    # Remove the file if it has an empty audio signal\n",
    "                    os.remove(file_path)\n",
    "                    print(f'Removed empty audio file: {file_path}')\n",
    "            except Exception as e:\n",
    "                print(f'Error loading audio file {file_path}: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d76e2d1",
   "metadata": {},
   "source": [
    "### Converting Mozilla to .wav\n",
    "\n",
    "The Mozilla dataset contains files that are of type .mp3, which will cause problems when we call the Librosa Library in the next section. We'll use Pydub to iterate through the Mozilla data and convert each file to .wav."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298615be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import os\n",
    "\n",
    "# Set the input and output directories\n",
    "input_dir = 'datasets/Audio/Mozilla_delta/'\n",
    "output_dir = 'datasets/Audio/Mozilla_delta(wav)//'\n",
    "\n",
    "# Loop through all .mp3 files in the input directory\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith('.mp3'):\n",
    "        # Load the audio file using pydub\n",
    "        audio = AudioSegment.from_file(os.path.join(input_dir, filename), format='mp3')\n",
    "\n",
    "        # Set the output filename and path\n",
    "        output_filename = filename[:-4] + '.wav' # Remove the '.mp3' extension and replace with '.wav'\n",
    "        output_path = os.path.join(output_dir, output_filename)\n",
    "\n",
    "        # Export the audio file in .wav format\n",
    "        audio.export(output_path, format='wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f60b3c9",
   "metadata": {},
   "source": [
    "### Image Compression\n",
    "\n",
    "To reduce the size of our image, we'll use PIL (Pillow) Fork to optimize the quality of our images. By setting quality to 85% and optimize = True, we shouldn't lose a considerable amount of data and we should be able to improve our model run time significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb456f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▍                                                                                           | 756/48273 [00:45<47:54, 16.53it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     12\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, filename))\n\u001b[0;32m---> 13\u001b[0m     \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquality\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquality\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/PIL/Image.py:2320\u001b[0m, in \u001b[0;36mImage.save\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2317\u001b[0m         fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw+b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2319\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2320\u001b[0m     \u001b[43msave_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2321\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   2322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m open_fp:\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/PIL/PngImagePlugin.py:1374\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(im, fp, filename, chunk, save_all)\u001b[0m\n\u001b[1;32m   1372\u001b[0m     _write_multiple_frames(im, fp, chunk, rawmode)\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1374\u001b[0m     \u001b[43mImageFile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_idat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrawmode\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info:\n\u001b[1;32m   1377\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m info_chunk \u001b[38;5;129;01min\u001b[39;00m info\u001b[38;5;241m.\u001b[39mchunks:\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/PIL/ImageFile.py:518\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc:\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;66;03m# compress to Python file-compatible object\u001b[39;00m\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 518\u001b[0m         l, s, d \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    519\u001b[0m         fp\u001b[38;5;241m.\u001b[39mwrite(d)\n\u001b[1;32m    520\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m s:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "directory = \"datasets/Spectrograms_Low/Synthetic_Crop/\"\n",
    "\n",
    "# Compression quality\n",
    "quality = 90\n",
    "\n",
    "# Count the number of files in the directory\n",
    "num_files = len(os.listdir(directory))\n",
    "\n",
    "# Iterate through the files with a progress bar\n",
    "for i, filename in tqdm(enumerate(os.listdir(directory)), total=num_files):\n",
    "    if filename.endswith(\".png\"):\n",
    "        img = Image.open(os.path.join(directory, filename))\n",
    "        img.save(os.path.join(directory, filename), optimize=True, quality=quality)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824bbdff",
   "metadata": {},
   "source": [
    "### PNG to JPEG Conversion\n",
    "\n",
    "Spectrogram images are typically composed of large regions of similar colors, which makes them well-suited for JPEG compression. JPEG is a lossy compression format that works well for images with smooth color gradients, such as photographs or images with natural scenes. Spectrogram images, which typically have few colors and smooth transitions between them, can also benefit from JPEG compression because the compression algorithm can identify and discard redundant information without significantly affecting the image quality.\n",
    "\n",
    "By compressing the spectrogram images using JPEG, you can significantly reduce the size of the image files without losing too much information. This can save disk space and reduce the amount of time needed to load the data into memory during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5afd043",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"datasets/Spectrograms_Low/Human_Crop/\"\n",
    "output_dir = \"datasets/Spectrograms_Low/hum_jpg\"\n",
    "quality = 100 # JPEG quality level (0-100)\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith('.png'):\n",
    "        input_path = os.path.join(input_dir, filename)\n",
    "        output_path = os.path.join(output_dir, os.path.splitext(filename)[0] + \".jpg\")\n",
    "        with Image.open(input_path) as im:\n",
    "            im.convert('RGB').save(output_path, 'JPEG', quality=quality)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae55e6c",
   "metadata": {},
   "source": [
    "### Tuning Quality\n",
    "\n",
    "This final tool will allow us to tune the quality of each individual JPG file. This will be balance between data clarity and data quality, as the hardware limitations of our system prohibit exceptionally large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e98588ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"datasets/Spectrograms_Low/hum_jpeg/\"\n",
    "output_dir = \"datasets/Spectrograms_Low/hum_jpeg/\"\n",
    "quality = 90 # JPEG quality level (0-100)\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith('.jpg'):\n",
    "        input_path = os.path.join(input_dir, filename)\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        with Image.open(input_path) as im:\n",
    "            im.convert('RGB').save(output_path, 'JPEG', quality=quality)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
