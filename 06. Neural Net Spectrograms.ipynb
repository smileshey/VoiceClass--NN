{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07c984ae",
   "metadata": {},
   "source": [
    "# Generating Spectrograms\n",
    "\n",
    "Now we'll be generating spectrograms for each audio  file. We'll be passing this spectrogram image into a CNN in the next worksheet, which will then classify the audio based on the content of this output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24c16e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import random\n",
    "\n",
    "\n",
    "from pandas import json_normalize\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score,classification_report\n",
    "from sklearn.preprocessing import normalize,StandardScaler,RobustScaler,MinMaxScaler\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "from scipy.stats import skew"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1d02da",
   "metadata": {},
   "source": [
    "## Librosa Spectrogram Call\n",
    "\n",
    "We'll use the two functions below to generate the Mel Spectrograms from our audio data. This code heavily utilizes the work of Jeff Prosise, who had previously created a similar workflow for his project to classify audio in the Amazon Rainforest. A reference to his project can be found at #11 in my references list.\n",
    "\n",
    "Once again, we'll be using the Librosa library to generate this data from the audio files in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "284804b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spectrogram(audio_file, image_file):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1)\n",
    "\n",
    "    y, sr = librosa.load(audio_file)\n",
    "    mel_spec = librosa.feature.melspectrogram(y, sr=sr)\n",
    "    log_mel = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    librosa.display.specshow(log_mel, sr=sr)\n",
    "\n",
    "    fig.savefig(image_file)\n",
    "    plt.close(fig)\n",
    "\n",
    "def create_pngs_from_wavs(input_path, output_path):\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "\n",
    "    processed_files = set()\n",
    "\n",
    "    for root, dirs, files in os.walk(input_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.wav'):\n",
    "                input_file = os.path.join(root, file)\n",
    "                output_file = os.path.join(output_path, file.strip('.wav') + '.png')\n",
    "                if input_file not in processed_files and not os.path.exists(output_file):\n",
    "                    create_spectrogram(input_file, output_file)\n",
    "                    processed_files.add(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9323d2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_pngs_from_wavs('datasets/Audio/Aptly_real/', 'datasets/Spectrograms/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fae883",
   "metadata": {},
   "source": [
    "### Cleaning Label Filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55ea21e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels = pd.DataFrame(pd.read_csv('datasets/Metadata/ModelFilePairs/model_file_pairs.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e36424cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels = df_labels.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "309c2fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels['filename'] = df_labels['filename'].str.rstrip('.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df4dcb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels['filename'] = df_labels['filename'].str.rstrip('.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b701b20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels['filename'] = df_labels['filename']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b57c5d47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          4198_61336_000001_000002\n",
       "1         8555_284447_000010_000001\n",
       "2         8455_210777_000076_000000\n",
       "3          6432_63722_000062_000000\n",
       "4          6432_63722_000073_000002\n",
       "                    ...            \n",
       "233424    reportorial_2011_0333_096\n",
       "233425            conv_2007_0031_10\n",
       "233426         conv_2009_0012-3_021\n",
       "233427          novel_2008_0049_084\n",
       "233428           news_2010_0026_054\n",
       "Name: filename, Length: 138869, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labels['filename']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6497b68",
   "metadata": {},
   "source": [
    "### Converting Spectrograms to Arrays & Labelling\n",
    "\n",
    "The label of '0' corresponds to human speech, '1' corresponds to synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d8cbf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "def load_images_from_path(path, label):\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for file in os.listdir(path):\n",
    "        if not file.endswith('.DS_Store'):  # exclude hidden macOS files\n",
    "            img = tf.keras.preprocessing.image.load_img(os.path.join(path, file), target_size=(224, 224))\n",
    "            img_array = np.array(img)\n",
    "            images.append(img_array)\n",
    "            labels.append((label))\n",
    "        \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4ab34e",
   "metadata": {},
   "source": [
    "The dataset is large enough to reduce in size. We'll random sample only a portion of the data, making sure to maintain the 66% majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e88db96",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_path = 'datasets/Spectrograms/HUMAN/'\n",
    "synthetic_path = 'datasets/Spectrograms/SYNTHETIC/'\n",
    "\n",
    "human_images, human_labels = load_images_from_path(human_path, 0)\n",
    "synthetic_images, synthetic_labels = load_images_from_path(synthetic_path, 1)\n",
    "\n",
    "# randomly sample X % of the data, maintaining 67% of the human images and 33% of the synthetic images\n",
    "human_sample_size = int(len(human_images) * 0.35)\n",
    "synthetic_sample_size = int(len(synthetic_images) * 0.22)\n",
    "\n",
    "human_sampled_indices = random.sample(range(len(human_images)), human_sample_size)\n",
    "human_sampled_images = [human_images[i] for i in human_sampled_indices]\n",
    "human_sampled_labels = [0] * human_sample_size\n",
    "\n",
    "synthetic_sampled_indices = random.sample(range(len(synthetic_images)), synthetic_sample_size)\n",
    "synthetic_sampled_images = [synthetic_images[i] for i in synthetic_sampled_indices]\n",
    "synthetic_sampled_labels = [1] * synthetic_sample_size\n",
    "\n",
    "x = human_sampled_images + synthetic_sampled_images\n",
    "y = human_sampled_labels + synthetic_sampled_labels\n",
    "\n",
    "X = np.array(x)\n",
    "Y = np.array(y).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2c22b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X : (41709, 224, 224, 3)\n",
      "Y: (41709, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"X :\", X.shape)\n",
    "print(\"Y:\", Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c501959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[[  0,   0,   2],\n",
       "          [  0,   0,   3],\n",
       "          [  0,   0,   3],\n",
       "          ...,\n",
       "          [  0,   0,   3],\n",
       "          [  0,   0,   3],\n",
       "          [  0,   0,   3]],\n",
       " \n",
       "         [[  0,   0,   3],\n",
       "          [  0,   0,   4],\n",
       "          [  0,   0,   4],\n",
       "          ...,\n",
       "          [  0,   0,   4],\n",
       "          [  0,   0,   4],\n",
       "          [  0,   0,   4]],\n",
       " \n",
       "         [[  0,   0,   3],\n",
       "          [  0,   0,   4],\n",
       "          [  0,   0,   4],\n",
       "          ...,\n",
       "          [  0,   0,   4],\n",
       "          [  0,   0,   4],\n",
       "          [  0,   0,   4]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[  0,   0,   3],\n",
       "          [  0,   0,   4],\n",
       "          [  0,   0,   4],\n",
       "          ...,\n",
       "          [  0,   0,   4],\n",
       "          [  0,   0,   4],\n",
       "          [  0,   0,   4]],\n",
       " \n",
       "         [[  0,   0,   3],\n",
       "          [  0,   0,   4],\n",
       "          [  0,   0,   4],\n",
       "          ...,\n",
       "          [  0,   0,   4],\n",
       "          [  0,   0,   4],\n",
       "          [  0,   0,   4]],\n",
       " \n",
       "         [[  0,   0,   3],\n",
       "          [  0,   0,   4],\n",
       "          [  0,   0,   4],\n",
       "          ...,\n",
       "          [  0,   0,   4],\n",
       "          [  0,   0,   4],\n",
       "          [  0,   0,   4]]],\n",
       " \n",
       " \n",
       "        [[[  0,   0,   2],\n",
       "          [  0,   0,   3],\n",
       "          [  0,   0,   3],\n",
       "          ...,\n",
       "          [  0,   0,   3],\n",
       "          [  0,   0,   3],\n",
       "          [  0,   0,   3]],\n",
       " \n",
       "         [[  0,   0,   3],\n",
       "          [  0,   0,   4],\n",
       "          [  0,   0,   4],\n",
       "          ...,\n",
       "          [  0,   0,   4],\n",
       "          [  0,   0,   4],\n",
       "          [  0,   0,   4]],\n",
       " \n",
       "         [[  0,   0,   3],\n",
       "          [  0,   0,   4],\n",
       "          [  0,   0,   4],\n",
       "          ...,\n",
       "          [  0,   0,   4],\n",
       "          [  0,   0,   4],\n",
       "          [  0,   0,   4]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[  0,   0,   3],\n",
       "          [  0,   0,   4],\n",
       "          [  0,   0,   4],\n",
       "          ...,\n",
       "          [  0,   0,   4],\n",
       "          [  0,   0,   4],\n",
       "          [  0,   0,   4]],\n",
       " \n",
       "         [[  0,   0,   3],\n",
       "          [  0,   0,   4],\n",
       "          [  0,   0,   4],\n",
       "          ...,\n",
       "          [  0,   0,   4],\n",
       "          [  0,   0,   4],\n",
       "          [  0,   0,   4]],\n",
       " \n",
       "         [[  0,   0,   3],\n",
       "          [  0,   0,   4],\n",
       "          [  0,   0,   4],\n",
       "          ...,\n",
       "          [  0,   0,   4],\n",
       "          [  0,   0,   4],\n",
       "          [  0,   0,   4]]],\n",
       " \n",
       " \n",
       "        [[[  0,   0,   2],\n",
       "          [  0,   0,   3],\n",
       "          [  0,   0,   3],\n",
       "          ...,\n",
       "          [  0,   0,   3],\n",
       "          [  0,   0,   3],\n",
       "          [  0,   0,   3]],\n",
       " \n",
       "         [[  0,   0,   3],\n",
       "          [  0,   0,   4],\n",
       "          [  0,   0,   4],\n",
       "          ...,\n",
       "          [  0,   0,   4],\n",
       "          [  0,   0,   4],\n",
       "          [  0,   0,   4]],\n",
       " \n",
       "         [[  0,   0,   3],\n",
       "          [  0,   0,   4],\n",
       "          [  0,   0,   4],\n",
       "          ...,\n",
       "          [  0,   0,   4],\n",
       "          [  0,   0,   4],\n",
       "          [  0,   0,   4]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[  0,   0,   5],\n",
       "          [  1,   1,   6],\n",
       "          [ 13,  10,  41],\n",
       "          ...,\n",
       "          [ 18,  13,  49],\n",
       "          [ 16,  11,  45],\n",
       "          [ 16,  11,  45]],\n",
       " \n",
       "         [[  4,   3,  20],\n",
       "          [  5,   4,  22],\n",
       "          [ 18,  13,  49],\n",
       "          ...,\n",
       "          [  0,   0,   4],\n",
       "          [  0,   0,   4],\n",
       "          [  0,   0,   4]],\n",
       " \n",
       "         [[  4,   3,  20],\n",
       "          [  5,   4,  22],\n",
       "          [ 18,  13,  49],\n",
       "          ...,\n",
       "          [  0,   0,   4],\n",
       "          [  0,   0,   4],\n",
       "          [  0,   0,   4]]],\n",
       " \n",
       " \n",
       "        ...,\n",
       " \n",
       " \n",
       "        [[[  0,   0,   2],\n",
       "          [  0,   0,   3],\n",
       "          [  0,   0,   3],\n",
       "          ...,\n",
       "          [  0,   0,   3],\n",
       "          [  0,   0,   3],\n",
       "          [  0,   0,   3]],\n",
       " \n",
       "         [[  0,   0,   3],\n",
       "          [  0,   0,   4],\n",
       "          [  0,   0,   4],\n",
       "          ...,\n",
       "          [  0,   0,   4],\n",
       "          [  0,   0,   4],\n",
       "          [  0,   0,   4]],\n",
       " \n",
       "         [[  0,   0,   3],\n",
       "          [  0,   0,   4],\n",
       "          [  0,   0,   4],\n",
       "          ...,\n",
       "          [  0,   0,   4],\n",
       "          [  0,   0,   4],\n",
       "          [  0,   0,   4]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 16,  11,  44],\n",
       "          [ 47,  17,  99],\n",
       "          [ 71,  16, 120],\n",
       "          ...,\n",
       "          [ 71,  16, 120],\n",
       "          [ 73,  16, 120],\n",
       "          [ 42,  17,  92]],\n",
       " \n",
       "         [[  1,   1,  12],\n",
       "          [ 36,  18,  83],\n",
       "          [ 73,  16, 120],\n",
       "          ...,\n",
       "          [  6,   5,  26],\n",
       "          [ 22,  15,  59],\n",
       "          [ 17,  12,  47]],\n",
       " \n",
       "         [[  1,   1,  12],\n",
       "          [ 36,  18,  83],\n",
       "          [ 73,  16, 120],\n",
       "          ...,\n",
       "          [  6,   5,  26],\n",
       "          [ 22,  15,  59],\n",
       "          [ 17,  12,  47]]],\n",
       " \n",
       " \n",
       "        [[[  0,   0,   2],\n",
       "          [  0,   0,   3],\n",
       "          [  0,   0,   3],\n",
       "          ...,\n",
       "          [  0,   0,   3],\n",
       "          [  0,   0,   3],\n",
       "          [  0,   0,   3]],\n",
       " \n",
       "         [[  0,   0,   3],\n",
       "          [  0,   0,   4],\n",
       "          [  0,   0,   4],\n",
       "          ...,\n",
       "          [  0,   0,   4],\n",
       "          [  0,   0,   4],\n",
       "          [  0,   0,   4]],\n",
       " \n",
       "         [[  0,   0,   3],\n",
       "          [  0,   0,   4],\n",
       "          [  2,   2,  13],\n",
       "          ...,\n",
       "          [  0,   0,   4],\n",
       "          [  0,   0,   4],\n",
       "          [  0,   0,   4]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[  0,   0,   3],\n",
       "          [ 26,  16,  66],\n",
       "          [ 45,  17,  97],\n",
       "          ...,\n",
       "          [ 19,  13,  52],\n",
       "          [ 18,  13,  49],\n",
       "          [  7,   6,  28]],\n",
       " \n",
       "         [[  0,   0,   3],\n",
       "          [  5,   4,  22],\n",
       "          [ 17,  12,  47],\n",
       "          ...,\n",
       "          [  0,   0,   4],\n",
       "          [  1,   1,   6],\n",
       "          [  0,   0,   4]],\n",
       " \n",
       "         [[  0,   0,   3],\n",
       "          [  5,   4,  22],\n",
       "          [ 17,  12,  47],\n",
       "          ...,\n",
       "          [  0,   0,   4],\n",
       "          [  1,   1,   6],\n",
       "          [  0,   0,   4]]],\n",
       " \n",
       " \n",
       "        [[[  0,   0,   2],\n",
       "          [  0,   0,   3],\n",
       "          [  0,   0,   3],\n",
       "          ...,\n",
       "          [ 27,  16,  67],\n",
       "          [ 27,  16,  67],\n",
       "          [ 27,  16,  67]],\n",
       " \n",
       "         [[  0,   0,   3],\n",
       "          [  0,   0,   4],\n",
       "          [  0,   0,   4],\n",
       "          ...,\n",
       "          [ 29,  17,  71],\n",
       "          [ 29,  17,  71],\n",
       "          [ 29,  17,  71]],\n",
       " \n",
       "         [[  0,   0,   3],\n",
       "          [  0,   0,   4],\n",
       "          [  0,   0,   4],\n",
       "          ...,\n",
       "          [ 29,  17,  71],\n",
       "          [ 29,  17,  71],\n",
       "          [ 29,  17,  71]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[105,  29, 121],\n",
       "          [112,  31, 129],\n",
       "          [112,  31, 129],\n",
       "          ...,\n",
       "          [126,  36, 130],\n",
       "          [126,  36, 130],\n",
       "          [126,  36, 130]],\n",
       " \n",
       "         [[158,  47, 118],\n",
       "          [168,  50, 125],\n",
       "          [168,  50, 125],\n",
       "          ...,\n",
       "          [147,  43, 128],\n",
       "          [147,  43, 128],\n",
       "          [147,  43, 128]],\n",
       " \n",
       "         [[158,  47, 118],\n",
       "          [168,  50, 125],\n",
       "          [168,  50, 125],\n",
       "          ...,\n",
       "          [147,  43, 128],\n",
       "          [147,  43, 128],\n",
       "          [147,  43, 128]]]], dtype=uint8),\n",
       " array([[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        ...,\n",
       "        [1],\n",
       "        [1],\n",
       "        [1]]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab8ba8c",
   "metadata": {},
   "source": [
    "### Pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e66f8f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pickles/X.pkl', 'wb') as f:\n",
    "    pickle.dump(X, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39e65168",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pickles/Y.pkl', 'wb') as f:\n",
    "    pickle.dump(Y, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31082ad2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
